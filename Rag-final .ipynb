{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2c1756e-9849-4753-8138-bf5b3dcf5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb22bc0-a529-490e-a3e1-2dda8450e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs found: ['L05_WordSimilarity.pdf', 'L06_Embeddings_QA_Recommendations_visual_audio.pdf', 'L07_classic_OCR_image_procc_IR (1).pdf', 'L08_classic_Speech_Recognition_IR.pdf', 'L09_Web Search Deep Dive.pdf', 'L10_LanguageModels_LMMs.pdf', 'L11_IR_w_LLMs_N_RAGs.pdf', 'מטלת בית מספר 1.pdf', 'תרגול SPSS.pdf']\n",
      "Loaded docs: 9\n",
      "Sample:\n",
      " [L05_WordSimilarity.pdf | page 1] Information Retrieval - Word Similarity - WordNet - Word Vectors Development: Moshe Friedman Credits: Yoav Goldberg, Ido Dagan, Reut Tsarfaty , Moshe Koppel, Wei Song, David Bamman, Ed Grefenstette, Chris Manning, Tsvi Kuflik, Hinrich Schütze, Christina Lioma and more\n",
      "[L05_WordSimilarity.pdf | page 2] Information Retrieval - administration Moshe Friedman Email: mo\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import fitz\n",
    "\n",
    "PDFS = [f for f in os.listdir() if f.lower().endswith(\".pdf\")]\n",
    "print(\"PDFs found:\", PDFS)\n",
    "\n",
    "def pdf_to_text(path):\n",
    "    doc = fitz.open(path)\n",
    "    pages = []\n",
    "    for i in range(len(doc)):\n",
    "        t = doc[i].get_text(\"text\")\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "        if t:\n",
    "            pages.append(f\"[{os.path.basename(path)} | page {i+1}] {t}\")\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "docs = [pdf_to_text(p) for p in PDFS]\n",
    "print(\"Loaded docs:\", len(docs))\n",
    "print(\"Sample:\\n\", docs[0][:400] if docs else \"No docs loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683241a7-2110-49b5-9dcf-07f1fb5d00cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks (overlap): 223\n",
      "Chunks (no_overlap): 187\n",
      "Example overlap chunk:\n",
      " [L05_WordSimilarity.pdf | page 1] Information Retrieval - Word Similarity - WordNet - Word Vectors Development: Moshe Friedman Credits: Yoav Goldberg, Ido Dagan, Reut Tsarfaty , Moshe Koppel, Wei Song, David Bamman, Ed Grefenstette, Chris Manning, Tsvi Kuflik, Hinrich Schütze, Christina Lioma and mo\n"
     ]
    }
   ],
   "source": [
    "def chunk_no_overlap(txt, size=1200):\n",
    "    return [txt[i:i+size].strip() for i in range(0, len(txt), size) if txt[i:i+size].strip()]\n",
    "\n",
    "def chunk_with_overlap(txt, size=1200, overlap=200):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(txt):\n",
    "        block = txt[i:i+size].strip()\n",
    "        if block:\n",
    "            chunks.append(block)\n",
    "        i += size - overlap\n",
    "    return chunks\n",
    "\n",
    "def build_chunks(docs, method=\"overlap\"):\n",
    "    chunks = []\n",
    "    for d in docs:\n",
    "        if method == \"overlap\":\n",
    "            chunks.extend(chunk_with_overlap(d))\n",
    "        else:\n",
    "            chunks.extend(chunk_no_overlap(d))\n",
    "    return chunks\n",
    "\n",
    "chunks_overlap = build_chunks(docs, method=\"overlap\")\n",
    "chunks_no_ov   = build_chunks(docs, method=\"no_overlap\")\n",
    "\n",
    "print(\"Chunks (overlap):\", len(chunks_overlap))\n",
    "print(\"Chunks (no_overlap):\", len(chunks_no_ov))\n",
    "print(\"Example overlap chunk:\\n\", chunks_overlap[0][:300] if chunks_overlap else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61cb42e6-166b-4fa0-b09c-4caf50cfaf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def build_tfidf_index(chunks):\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
    "    X = tfidf.fit_transform(chunks)\n",
    "    return tfidf, X\n",
    "\n",
    "def retrieve_tfidf(tfidf, X, chunks, query, k=5):\n",
    "    q_vec = tfidf.transform([query])\n",
    "    sims = cosine_similarity(q_vec, X).flatten()\n",
    "    idx = np.argsort(sims)[::-1][:k]\n",
    "    return [(int(i), float(sims[i]), chunks[i]) for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b1cf19-7ff7-414e-92ca-b79de414c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def build_emb_index(chunks):\n",
    "    X = embed.encode(chunks, normalize_embeddings=True, show_progress_bar=True)\n",
    "    return X\n",
    "\n",
    "def retrieve_emb(X_emb, chunks, query, k=5):\n",
    "    qv = embed.encode([query], normalize_embeddings=True)[0]\n",
    "    sims = X_emb @ qv\n",
    "    idx = np.argsort(sims)[::-1][:k]\n",
    "    return [(int(i), float(sims[i]), chunks[i]) for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c83d904-3d35-4b32-8e70-e9758d898f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def ask_ollama(prompt, model=\"llama3.2:1b\"):\n",
    "    r = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n",
    "        timeout=120\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"response\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a8b69a6-550b-4237-90c5-108d42a044ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"model\":\"llama3.2:1b\",\"created_at\":\"2026-01-31T13:28:07.1259127Z\",\"response\":\"Hello.\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,46864,24748,128009,128006,78191,128007,271,9906,13],\"total_duration\":99\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\"model\":\"llama3.2:1b\",\"prompt\":\"Say hello\",\"stream\":False},\n",
    "    timeout=60\n",
    ")\n",
    "print(r.status_code)\n",
    "print(r.text[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "634f0de9-0fa4-418e-975a-f5a5d1a5965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.get(\"http://localhost:11434/api/tags\").status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2271823d-d706-4bc6-ad43-7aed36bdbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyBG-XYdUB21UfitSq-weEV-1ucx8lNrn-k\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "807e4f39-da43-4c1a-b08f-7c822e283187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "def ask_gemini(prompt, model=\"gemini-flash-latest\"):\n",
    "    resp = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt\n",
    "    )\n",
    "    return resp.text\n",
    "\n",
    "print(ask_gemini(\"Say hello\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05ec8dc3-fe8d-4fc8-8e20-b356c513d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llama3.2:1b', 'mistral:latest']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "tags = requests.get(\"http://localhost:11434/api/tags\").json()\n",
    "print([m[\"name\"] for m in tags.get(\"models\", [])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7519e7ab-88dd-4442-bb39-cf59d9ae6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(hits, max_chars=2500):\n",
    "    parts, total = [], 0\n",
    "    for i, score, txt in hits:\n",
    "        block = f\"[chunk_id={i} score={score:.4f}]\\n{txt}\\n\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        parts.append(block)\n",
    "        total += len(block)\n",
    "    return \"\\n---\\n\".join(parts)\n",
    "\n",
    "def build_prompt(context, question):\n",
    "    return f\"\"\"You are a QA assistant.\n",
    "Answer using ONLY the provided context.\n",
    "If the context is insufficient, answer briefly and say what is missing from the context.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "Return:\n",
    "1) Answer (short and clear)\n",
    "2) Evidence: list the chunk_id(s) you used\n",
    "\"\"\"\n",
    "\n",
    "def rag_answer(question, chunks, tfidf=None, X_tfidf=None, X_emb=None,\n",
    "               retriever=\"tfidf\", llm=\"local\", top_k=5, local_model=\"llama3.2:1b\"):\n",
    "\n",
    "    if retriever == \"tfidf\":\n",
    "        hits = retrieve_tfidf(tfidf, X_tfidf, chunks, question, k=top_k)\n",
    "    else:\n",
    "        hits = retrieve_emb(X_emb, chunks, question, k=top_k)\n",
    "\n",
    "    context = build_context(hits)\n",
    "    prompt = build_prompt(context, question)\n",
    "\n",
    "    if llm == \"local\":\n",
    "        answer = ask_ollama(prompt, model=local_model)\n",
    "    else:\n",
    "        answer = ask_gemini(prompt)\n",
    "\n",
    "    return hits, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d688bd-3655-48e4-b077-a1f2a425acf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aeaf6cc40ee44f9aad74317a9035c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db3e18fd2954c4e89109d74a6ff82da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "CHUNKING=overlap | RETRIEVER=tfidf | LLM=local\n",
      "Unfortunately, there is insufficient context to answer this question accurately. The provided text lacks information about why chunking is important in RAG systems or how it applies to specific tasks or scenarios.\n",
      "\n",
      "Missing from the context:\n",
      "\n",
      "* A clear explanation of what RAG system is and its purpose.\n",
      "* More details on the benefits or goals of using chunking in RAGs.\n",
      "* Information on specific task types (e.g., text summarization, question answering) where chunking would be useful.\n",
      "* More context about the different chunking techniques mentioned (e.g., document structure-based, semantic chunking).\n",
      "\n",
      "Therefore, I can only provide a brief answer:\n",
      "\n",
      "Top evidence chunks: [(199, 0.2157), (198, 0.2061), (196, 0.1736)]\n",
      "\n",
      "==========================================================================================\n",
      "CHUNKING=overlap | RETRIEVER=tfidf | LLM=external\n",
      "1) Answer (short and clear)\n",
      "The provided context is insufficient to explain the fundamental importance of chunking in RAG systems. The context describes different methods of chunking (semantic, adaptive, contextual) and notes that specific methods are useful for retaining meaning or context, especially in complex documents.\n",
      "\n",
      "2) Evidence: list the chunk_id(s) you used\n",
      "199, 198 (Used to confirm the information present)\n",
      "\n",
      "Top evidence chunks: [(199, 0.2157), (198, 0.2061), (196, 0.1736)]\n",
      "\n",
      "==========================================================================================\n",
      "CHUNKING=overlap | RETRIEVER=emb | LLM=local\n",
      "I'm a QA assistant.\n",
      "\n",
      "The context is insufficient to answer why chunking is important in RAG systems. Specifically, I am missing information about what \"RAG\" refers to or how it relates to the topic of chunking in text processing.\n",
      "\n",
      "Evidence: [chunk_id=196 score=0.5176]\n",
      "\n",
      "Top evidence chunks: [(196, 0.5176), (198, 0.5079), (208, 0.4158)]\n",
      "\n",
      "==========================================================================================\n",
      "CHUNKING=overlap | RETRIEVER=emb | LLM=external\n",
      "1) Answer (short and clear)\n",
      "Insufficient context. The context describes various chunking methods and the benefits of specific approaches, but it does not explicitly state the fundamental reason why chunking is important or necessary for RAG systems.\n",
      "\n",
      "2) Evidence: list the chunk_id(s) you used\n",
      "[196], [198]\n",
      "\n",
      "Top evidence chunks: [(196, 0.5176), (198, 0.5079), (208, 0.4158)]\n",
      "\n",
      "==========================================================================================\n",
      "CHUNKING=no_overlap | RETRIEVER=tfidf | LLM=local\n",
      "Missing from the context is an explanation of what \"RAG systems\" are, how they work, or why chunking is important in them. \n",
      "\n",
      "Answer:\n",
      "\n",
      "Chunking is essential in RAG systems because it helps identify semantic meaning within a document by breaking it into meaningful groups (chunks) of sentences that talk about the same theme or topic.\n",
      "\n",
      "Evidence: [chunk_id=166 score=0.2457]\n",
      "\n",
      "Top evidence chunks: [(166, 0.2457), (165, 0.2023), (164, 0.1774)]\n",
      "\n",
      "==========================================================================================\n",
      "CHUNKING=no_overlap | RETRIEVER=emb | LLM=local\n",
      "CHUNKING IS IMPORTANT IN RAG SYSTEMS AS IT HELPS PRESERVE THE ORIGINAL STRUCTURE OF CONTENT DURING CHUNKCreation.\n",
      "\n",
      "[chunk_id=164]\n",
      "[L11_IR_w_LLMs_N_RAGs.pdf | page 131] \n",
      "[L11_IR_w_LLMs_N_RAGs.pdf | page 132]\n",
      "\n",
      "Top evidence chunks: [(164, 0.5176), (165, 0.4187), (174, 0.4158)]\n"
     ]
    }
   ],
   "source": [
    "question = \"Why is chunking important in RAG systems?\"\n",
    "\n",
    "# נבנה אינדקסים עבור overlap\n",
    "tfidf_o, X_tfidf_o = build_tfidf_index(chunks_overlap)\n",
    "X_emb_o = build_emb_index(chunks_overlap)\n",
    "\n",
    "# נבנה אינדקסים עבור no-overlap\n",
    "tfidf_n, X_tfidf_n = build_tfidf_index(chunks_no_ov)\n",
    "X_emb_n = build_emb_index(chunks_no_ov)\n",
    "\n",
    "experiments = [\n",
    "    (\"overlap\", \"tfidf\", \"local\"),\n",
    "    (\"overlap\", \"tfidf\", \"external\"),\n",
    "    (\"overlap\", \"emb\",   \"local\"),\n",
    "    (\"overlap\", \"emb\",   \"external\"),\n",
    "    (\"no_overlap\", \"tfidf\", \"local\"),\n",
    "    (\"no_overlap\", \"emb\",   \"local\"),\n",
    "]\n",
    "\n",
    "for chunking, retr, llm in experiments:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"CHUNKING={chunking} | RETRIEVER={retr} | LLM={llm}\")\n",
    "\n",
    "    if chunking == \"overlap\":\n",
    "        chunks = chunks_overlap\n",
    "        tfidf, X_tfidf, X_emb = tfidf_o, X_tfidf_o, X_emb_o\n",
    "    else:\n",
    "        chunks = chunks_no_ov\n",
    "        tfidf, X_tfidf, X_emb = tfidf_n, X_tfidf_n, X_emb_n\n",
    "\n",
    "    hits, ans = rag_answer(\n",
    "        question,\n",
    "        chunks=chunks,\n",
    "        tfidf=tfidf, X_tfidf=X_tfidf, X_emb=X_emb,\n",
    "        retriever=retr, llm=llm,\n",
    "        top_k=5,\n",
    "        local_model=\"llama3.2:1b\"\n",
    "    )\n",
    "\n",
    "    print(ans)\n",
    "    print(\"\\nTop evidence chunks:\", [(i, round(s,4)) for i,s,_ in hits[:3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df02937-f0fa-46a0-84a5-5a51245745f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
